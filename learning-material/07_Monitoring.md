# Monitoring

These notes summarize the key concepts of In-band Network Telemetry (INT) as presented in the "Datacenter Network Programming – Summersemester 2023" lecture.

## 1. The Problem with Traditional Network Monitoring

Traditional network monitoring, even in SDN, has several limitations:

-   **Polling-based:** The control plane polls switches for counters (per-port, per-flow).
-   **Too Slow:** Network state can change much faster than the polling interval.
-   **High CPU Stress:** Fine-grained monitoring overloads switch CPUs.
-   **Loss of Granularity:** Sampling (sFlow/NetFlow) is used to reduce load, but this loses detailed information.
-   **Lacks End-to-End View:** Correlating data from multiple switches to trace a single packet's journey is difficult and often done offline.

## 2. In-band Network Telemetry (INT) - The "Million Minions" Approach

INT uses data packets themselves as "minions" to collect monitoring information directly from the data plane as they traverse the network.

-   **Core Idea:** Add telemetry metadata directly to live data packets.
-   **Line-Rate:** Collection happens in the hardware pipeline at line speed.
-   **Programmable:** P4 allows us to define precisely what data to collect.
-   **Rich Data:** Can collect path, latency, queue occupancy, matched rules, and more per-hop.

### INT Architecture Roles

1.  **INT Source:** The first switch on an INT-enabled path.
    -   Receives instructions from the Control Plane.
    -   Adds an **INT Header** to the packet, including an **instruction bitmask** that tells other switches what data to collect.
    -   Adds its own metadata (e.g., its Switch ID).

2.  **INT Transit:** Any intermediate switch.
    -   Reads the instruction bitmask.
    -   Appends its own telemetry data to the packet.

3.  **INT Sink:** The last switch on the path.
    -   Collects its own metadata.
    -   Strips the INT header and all collected metadata from the packet.
    -   Generates a **telemetry report** and sends it to a collector.
    -   Forwards the original packet to its destination.

4.  **INT Monitor:** A central collector.
    -   Receives, logs, and analyzes telemetry reports.
    -   Can be integrated with big data systems like Kafka and visualization tools like Kibana for real-time analysis.

## 3. INT Headers and Metadata Stack

-   **Encapsulation:** INT is not a protocol itself but a payload. It can be encapsulated within various protocols like **VXLAN-GPE**, **Geneve**, TCP/UDP, or GRE. All devices on the path must support the chosen encapsulation.
-   **Instruction Bitmask:** A key field in the INT header. Each bit corresponds to a specific piece of metadata to collect (e.g., bit 0=SwitchID, bit 1=Hop Latency).
-   **Metadata Stack:** Collected telemetry data is organized as a stack within the packet. Each INT switch **pushes** its metadata to the **front** of the stack (LIFO). This means the Sink receives a stack where the first metadata entry belongs to the last transit hop, and the last entry belongs to the Source.

### P4 Code Snippet Example (Adding Hop Latency)

```p4
// This action calculates latency and adds it to the front of the metadata stack
action add_int_hop_latency () {
    hdr.int_metadata.push_front(1); // Add space for 1 metadata item
    hdr.int_metadata[0].data = 
        (bit<32>)(meta.fwd_metadata.eg_timestamp - meta.fwd_metadata.ig_timestamp);
}
```

## 4. INT Report Types

INT systems can generate different reports based on network events:

-   **Local Flow Report:** End-to-end report for a specific, whitelisted flow.
-   **Drop Report:** Generated by any switch when a packet is dropped, providing the exact location and reason for the drop.
-   **Queue Congestion Report:** Sent when a queue's depth or latency exceeds a predefined threshold.

## 5. Challenges and Overhead

-   **The Main Challenge:** Deciding **Where, When, and How** to collect data. Monitoring everything all the time creates significant overhead.
-   **Overhead:** Adding telemetry data increases packet size, and processing it adds a small amount of latency at each hop.
-   **Performance:** Benchmarks show that latency overhead **scales linearly** with the number of telemetry fields collected. This overhead is predictable but must be managed.
-   **Closed-Loop Control:** The goal is an analytics framework where insights from telemetry (e.g., congestion detected) automatically trigger control-plane actions (e.g., reroute traffic).

## 6. Practical Example: Multi-Hop Route Inspection (MRI)

The programming assignment demonstrates a simplified version of INT.

-   **Goal:** Detect congestion on a bottleneck link.
-   **Method:**
    1.  Define custom `mri_t` and `switch_t` headers in P4.
    2.  In the egress pipeline of each switch, write P4 code to:
        - Increment a hop counter (`mri.count`).
        - Push a new `switch_t` header onto a header stack.
        - Populate it with the `switchID` and current `deq_qdepth` (dequeue queue depth).
        - Update the outer IPv4 header's total length.
    3.  In the parser, use the `mri.count` to loop and parse all collected `switch_t` headers.
-   **Result:** Packets arriving at the destination contain a trace of the path, including the queue depth at each hop, clearly identifying the point of congestion.

## 7. Related Advanced Telemetry Concepts

INT is part of a broader field of programmable monitoring. Other important concepts include:

-   **Query-Driven (Sonata/Marple):** Users write high-level queries (e.g., "find all flows with >10ms latency"). A compiler automatically generates the necessary P4 and control plane code.
-   **Sketch-Based (FlowRadar/UnivMon):** Use probabilistic data structures (sketches) in the data plane to track properties of *all* flows with very low memory, trading perfect accuracy for broad coverage. Ideal for finding heavy hitters or scanning activity.
-   **Flow Diagnosis (Dapper):** Special-purpose P4 programs designed to diagnose specific issues, like identifying the root cause of poor TCP performance (sender, network, or receiver limited).
# Study Notes: Datacenter Network Programming - In-Network Monitoring (Telemetry)

These notes summarize the key concepts of In-band Network Telemetry (INT) as presented in the "Datacenter Network Programming – Summersemester 2023" lecture.

## 1. The Problem with Traditional Network Monitoring

Traditional network monitoring, even in SDN, has several limitations:

-   **Polling-based:** The control plane polls switches for counters (per-port, per-flow).
-   **Too Slow:** Network state can change much faster than the polling interval.
-   **High CPU Stress:** Fine-grained monitoring overloads switch CPUs.
-   **Loss of Granularity:** Sampling (sFlow/NetFlow) is used to reduce load, but this loses detailed information.
-   **Lacks End-to-End View:** Correlating data from multiple switches to trace a single packet's journey is difficult and often done offline.

## 2. In-band Network Telemetry (INT) - The "Million Minions" Approach

INT uses data packets themselves as "minions" to collect monitoring information directly from the data plane as they traverse the network.

-   **Core Idea:** Add telemetry metadata directly to live data packets.
-   **Line-Rate:** Collection happens in the hardware pipeline at line speed.
-   **Programmable:** P4 allows us to define precisely what data to collect.
-   **Rich Data:** Can collect path, latency, queue occupancy, matched rules, and more per-hop.

### INT Architecture Roles

1.  **INT Source:** The first switch on an INT-enabled path.
    -   Receives instructions from the Control Plane.
    -   Adds an **INT Header** to the packet, including an **instruction bitmask** that tells other switches what data to collect.
    -   Adds its own metadata (e.g., its Switch ID).

2.  **INT Transit:** Any intermediate switch.
    -   Reads the instruction bitmask.
    -   Appends its own telemetry data to the packet.

3.  **INT Sink:** The last switch on the path.
    -   Collects its own metadata.
    -   Strips the INT header and all collected metadata from the packet.
    -   Generates a **telemetry report** and sends it to a collector.
    -   Forwards the original packet to its destination.

4.  **INT Monitor:** A central collector.
    -   Receives, logs, and analyzes telemetry reports.
    -   Can be integrated with big data systems like Kafka and visualization tools like Kibana for real-time analysis.

## 3. INT Headers and Metadata Stack

-   **Encapsulation:** INT is not a protocol itself but a payload. It can be encapsulated within various protocols like **VXLAN-GPE**, **Geneve**, TCP/UDP, or GRE. All devices on the path must support the chosen encapsulation.
-   **Instruction Bitmask:** A key field in the INT header. Each bit corresponds to a specific piece of metadata to collect (e.g., bit 0=SwitchID, bit 1=Hop Latency).
-   **Metadata Stack:** Collected telemetry data is organized as a stack within the packet. Each INT switch **pushes** its metadata to the **front** of the stack (LIFO). This means the Sink receives a stack where the first metadata entry belongs to the last transit hop, and the last entry belongs to the Source.

### P4 Code Snippet Example (Adding Hop Latency)

```p4
// This action calculates latency and adds it to the front of the metadata stack
action add_int_hop_latency () {
    hdr.int_metadata.push_front(1); // Add space for 1 metadata item
    hdr.int_metadata[0].data = 
        (bit<32>)(meta.fwd_metadata.eg_timestamp - meta.fwd_metadata.ig_timestamp);
}
```

## 4. INT Report Types

INT systems can generate different reports based on network events:

-   **Local Flow Report:** End-to-end report for a specific, whitelisted flow.
-   **Drop Report:** Generated by any switch when a packet is dropped, providing the exact location and reason for the drop.
-   **Queue Congestion Report:** Sent when a queue's depth or latency exceeds a predefined threshold.

## 5. Challenges and Overhead

-   **The Main Challenge:** Deciding **Where, When, and How** to collect data. Monitoring everything all the time creates significant overhead.
-   **Overhead:** Adding telemetry data increases packet size, and processing it adds a small amount of latency at each hop.
-   **Performance:** Benchmarks show that latency overhead **scales linearly** with the number of telemetry fields collected. This overhead is predictable but must be managed.
-   **Closed-Loop Control:** The goal is an analytics framework where insights from telemetry (e.g., congestion detected) automatically trigger control-plane actions (e.g., reroute traffic).

## 6. Practical Example: Multi-Hop Route Inspection (MRI)

The programming assignment demonstrates a simplified version of INT.

-   **Goal:** Detect congestion on a bottleneck link.
-   **Method:**
    1.  Define custom `mri_t` and `switch_t` headers in P4.
    2.  In the egress pipeline of each switch, write P4 code to:
        - Increment a hop counter (`mri.count`).
        - Push a new `switch_t` header onto a header stack.
        - Populate it with the `switchID` and current `deq_qdepth` (dequeue queue depth).
        - Update the outer IPv4 header's total length.
    3.  In the parser, use the `mri.count` to loop and parse all collected `switch_t` headers.
-   **Result:** Packets arriving at the destination contain a trace of the path, including the queue depth at each hop, clearly identifying the point of congestion.

## 7. Related Advanced Telemetry Concepts

INT is part of a broader field of programmable monitoring. Other important concepts include:

-   **Query-Driven (Sonata/Marple):** Users write high-level queries (e.g., "find all flows with >10ms latency"). A compiler automatically generates the necessary P4 and control plane code.
-   **Sketch-Based (FlowRadar/UnivMon):** Use probabilistic data structures (sketches) in the data plane to track properties of *all* flows with very low memory, trading perfect accuracy for broad coverage. Ideal for finding heavy hitters or scanning activity.
-   **Flow Diagnosis (Dapper):** Special-purpose P4 programs designed to diagnose specific issues, like identifying the root cause of poor TCP performance (sender, network, or receiver limited).
